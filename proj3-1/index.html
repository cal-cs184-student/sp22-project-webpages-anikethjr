<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<style>  
    div.padded {  
      padding-top: 0px;  
      padding-right: 100px;  
      padding-bottom: 0.25in;  
      padding-left: 100px;  
    }  
  </style> 
<title>Your Name  |  CS 184</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link rel="stylesheet" type="text/css" href="style.css" media="screen" />
</head>
<body>
<br />
<h1 align="middle">Assignment 3: PathTracer</h1>
    <h2 align="middle">Aniketh Janardhan Reddy and Aditya Ramkumar</h2>

    <div class="padded">
        <p>Use this section to write an overview of the assignment. All of the text in your write-up should be <em>in your own words</em>. If you need to add additional HTML features to this document, you can search the <a href="http://www.w3schools.com/">http://www.w3schools.com/</a> website for instructions. To edit the HTML, you can just copy and paste existing chunks and fill in the text and image file names appropriately.</p>
        <o>The website writeup is intended to be a self-contained walkthrough of the assignment: we want this to be a piece of work which showcases your understanding of relevant concepts through both mesh images as well as written explanations about what you did to complete each part of the assignment. Try to be as clear and organized as possible when writing about your own output files or extensions to the assignment. We want to understand what you've achieved and how you've done it!</p> 
        <p>If you are well-versed in web development, feel free to ditch this template and make a better looking page. Just make sure that you include all the components as we've laid them out here. </p>

    <h2 align="middle">Part 1: Ray Generation and Intersection</h2>
        Part 1
Walk through the ray generation and primitive intersection parts of the rendering pipeline.
Explain the triangle intersection algorithm you implemented in your own words.
Show images with normal shading for a few small .dae files.
        
        <h3>Ray Generation</h3>

        <p>The ray generation part of the rendering pipeline starts off by sampling ns_aa number of rays at every pixel. Every ray starts at the camera and is directed from the camera to a position on the sensor. The camera is at a fixed location - the origin in camera space and at Camera->pos in the world space. The sensor is located along some predefined plane. At every given pixel, a ray is generated by:</p>

        <ol>
            <li>Sampling uniformly over a unit 2D grid and then adding the randomly sampled value to the pixel's coordinates to get the direction of the ray. Note that this sampled direction only has x and y components. The z component is computed in the next steps.</li>
            <li>The direction is then converted to normalized image coordinates by dividing the x-dimension by the width of the image W and dividing the y-dimension by the height of the image H.</li>
            <li>Then, the normalized image space direction is converted to camera space coordinates by (note that the z component of the direction is added here):
                <ol>
                    <li>x = (x-0.5)*(tan(hFov/2)*2.0)</li>
                    <li>y = (y-0.5)*(tan(vFov/2)*2.0)</li>
                    <li>z = -1 (the camera is looking down the -Z direction and the sensor is along the Z=-1 plane)</li>
                </ol>
                The direction vector is now complete and a ray can be constructed in camera space - its starting point is the origin and its direction is given by the unit vector pointing from the origin to (x, y, z).
            </li>
            <li>Finally, the ray is transformed back to world space by first rotating it using the camera-to-world rotation matrix and then translating by the position of the camera in world space.</li>
        </ol>

        <p>After generating a ray, we estimate its radiance. The Monte Carlo estimate of the integral of radiance over a given pixel is computed by averaging the estimated radiance along each ray sampled for that pixel.</p>

        <h3>Primitive Intersection</h3>

        <p>To estimate the radiance along each ray, we need to find intersections of the ray with scene primitives. In this part, we computed the intersection points of a ray with triangle and sphere primitives. If a ray hits a primitive, the intersection point and its distance from the ray's starting point (t-value) is returned along with other details about the primitive such as its type, BSDF and the surface normal at the point of intersection. For a ray, we are usually interested in its first intersection with any scene primitive since the light from this point is what is captured by the camera. Thus, when a ray intersects with a primitive, we update the "ending point" of the ray so as to keep track of the first intersection. Any intersection that is beyond the ending point is not considered a valid intersection. </p>

        <h3>Triangle Intersection Algorithm</h3>

        <p>We used the Moller-Trumbore intersection algorithm to find the intersection of a ray with a triangle. The algorithm computes the solution to the following equation:</p>

        <p>o + t*d = (1 - b1 - b2)*p0 + b1*p1 + b2*p2</p>

        <p>where o is the ray's starting point, d is the ray's direction, t is the distance of the intersection from o along d and p0, p1 and p2 are the vertices of the triangle. (1-b1-b2), b1 and b2 are the barycentric coordinates of the intersection point. Solving this equation for t, b1 and b2 gives us not only the intersection point but also the barycentric coordinates of the intersection. Analytically, the solution to this equation is:</p>

        <p> t = (1 / (S1.E1)) * (S2.E2) </p>
        <p> b1 = (1 / (S1.E1)) * (S1.S) </p>
        <p> b2 = (1 / (S1.E1)) * (S2.d) </p>

        <p>where,</p>
        <p>E1 = p1 - p0</p>
        <p>E2 = p2 - p0</p>
        <p>S = o - p0</p>
        <p>S1 = d x E2</p>
        <p>S2 = S x E1</p>

        <p>For an intersection to be valid, the t computed using the above equations must be >= 0 and <= to the distance from o to the ending point of the ray. Also, 0<=b1, b2, (1-b1-b2)<=1. </p>

        <h3> Images with Normal Shading </h3>

        <div align="center">
            <table style="width=100%">
                <tr>
                    <td align="middle">
                    <img src="images/CBempty.png" width="480px" />
                    <figcaption align="middle">Rendering of CBempty.dae with normal shading</figcaption>
                </tr>
                <tr>
                    <td align="middle">
                    <img src="images/CBspheres.png" width="480px" />
                    <figcaption align="middle">Rendering of CBspheres.dae with normal shading</figcaption>
                </tr>
            </table>
        </div>


    <h2 align="middle">Part 2: Bounding Volume Hierarchy</h2>
        <p>In the second part of our project, we implemented a Bounding Volume Hierarchy (BVH). This is essentially a series of nested bounding boxes, which helps us avoid checking all primitives in a scene during ray tracing. In order to implement this, we first created a bounding box that contained all of the primitives. Then recursively, we used a heuristic to dispatch two child nodes (left and right) of our root to split the remaining primitives - we did this as evenly as possible. </p>
        <p>What heuristic did we use? We settled on caclulating the average bounding box centroid for our elements, and using the difference in # of primitives as the heuristic. We calculated the split along all axes, and split the primitives along the most even one. </p>
        <p>Here are a couple renderings of normal shading for a few large .dae files that we can only render with BVH acceleration (will take a long time otherwise!)</p>
        <div align="center">
            <table style="width=100%">
                <tr>
                    <td align="middle">
                    <img src="images/CBLucy.png" width="480px" />
                    <figcaption align="middle">CBLucy.dae takes 0.1286s to render</figcaption>
                </tr>
                <tr>
                    <td align="middle">
                    <img src="images/maxplank.png" width="480px" />
                    <figcaption align="middle">maxplank.dae takes 0.2009s to render</figcaption>
                </tr>
            </table>
        </div>
        <p>The logarithimic runtime of using BVH is evident in the results of rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. The banana.dae file had 2458 primitives and took 30.39 seconds without BVH, and took 0.1639 seconds with BVH. Similarly, the cow.dae file had 5856 primitives took 83.23 seconds without BVH, and took an average of 0.2056 seconds with BVH. Furthermore, larger images take significantly longer to render without BVH to the point where it's infeasable to not use it.</p>
        <div align="center">
            <table style="width=100%">
                <tr>
                    <td align="middle">
                    <img src="images/cow.png" width="480px" />
                    <figcaption align="middle">Rendering of cow.dae</figcaption>
                </tr>
                <tr>
                    <td align="middle">
                    <img src="images/banana.png" width="480px" />
                    <figcaption align="middle">Rendering of banana.dae</figcaption>
                </tr>
            </table>
        </div>

    <h2 align="middle">Part 5: Adaptive Sampling</h2>
        <p>In the fifth part of our project, we implemented adaptive sampling. The premise behind this is that Monte Carlo path tracing results in a lot of noise, which can be eliminated by increasing the number of samples per pixel. Since this is computationally expensive and takes time, we can speed up the process by concentrating our samples on areas that converge slower and quitting early when we encounter areas that converge faster.</p>
        <p>We implemented this by defining a variable I to be 1.96 * sqrt(sigma^2 / n). We then compare the value I to mu * maxTolerance in order to determine if a pixel has converged. As described in the project description, these values can be computed more efficiently by storing a running sum of the illuminance values (and a running sum of the squares of these illuminance values). And to avoid repeated computation, we only do this check once every `sampleCountBuffer` times.</p>
        <p>Here is the rendering for bunny.dae scene with adaptive sampling. The parameters we used were 2048 samples per pixel, 1 sample per light, and a max ray depth of 5.</p>
        <div align="center">
            <table style="width=100%">
                <tr>
                    <td align="middle">
                    <img src="images/bunny_rate.png" width="480px" />
                    <figcaption align="middle">Sample rate image for bunny.dae</figcaption>
                </tr>
                <tr>
                    <td align="middle">
                    <img src="images/bunny.png" width="480px" />
                    <figcaption align="middle">Noise-free rendered result for bunny.dae</figcaption>
                </tr>
            </table>
        </div>
        



    <h2 align="middle">A Few Notes On Webpages</h2>
        <p>Here are a few problems students have encountered in the past. You will probably encounter these problems at some point, so don't wait until right before the deadline to check that everything is working. Test your website on the instructional machines early!</p>
        <ul>
        <li>Your main report page should be called index.html.</li>
        <li>Be sure to include and turn in all of the other files (such as images) that are linked in your report!</li>
        <li>Use only <em>relative</em> paths to files, such as <pre>"./images/image.jpg"</pre>
        Do <em>NOT</em> use absolute paths, such as <pre>"/Users/student/Desktop/image.jpg"</pre></li>
        <li>Pay close attention to your filename extensions. Remember that on UNIX systems (such as the instructional machines), capitalization matters. <pre>.png != .jpeg != .jpg != .JPG</pre>
        <li>Be sure to adjust the permissions on your files so that they are world readable. For more information on this please see this tutorial: <a href="http://www.grymoire.com/Unix/Permissions.html">http://www.grymoire.com/Unix/Permissions.html</a></li>
        <li>And again, test your website on the instructional machines early!</li>
</div>
</body>
</html>




